{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARFLo3zd956h"
      },
      "source": [
        "*  Paa Kwesi Jnr Thompson\n",
        "*  Isaac Baah\n",
        "*  Emmanuel Nhyira Freduah-Agyemang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsoFmOaHMug6"
      },
      "source": [
        "# Checking for GPU Availability\n",
        "Before starting the model training, we checked if a GPU was available using the nvidia-smi command. This command gives details about any connected NVIDIA GPUs, such as memory usage, temperature, and running processes. The code checks the output for the word \"failed\" to confirm whether a GPU is accessible. If no GPU is detected, a message is displayed to let us know. Otherwise, the GPU details are printed.\n",
        "\n",
        "This step is important because deep learning models require a lot of computational power, and training on a GPU is much faster than using a CPU. By checking for a GPU at the start, we made sure that the hardware we needed for the project was available before moving on to training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apMsgNmUK-ds"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vfSZr3mYPUO"
      },
      "source": [
        "Earlier in the project, we faced repeated system crashes due to resource exhaustion when training on a CPU instead of a GPU. These crashes slowed our progress and highlighted the importance of using the appropriate hardware. By confirming the availability of a high-performance GPU, we ensured that the training process would be stable and efficient, allowing us to proceed confidently with the next stages of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVUDmPXjMxVB"
      },
      "source": [
        "# Installing Necessary Libraries\n",
        "As part of setting up the environment for our project, we upgraded and installed essential Python libraries using pip. The first command upgraded the pip package installer itself to the latest version, ensuring compatibility with the latest features and dependencies required by modern libraries. This step is critical because outdated versions of pip can lead to installation errors or incompatibility issues.\n",
        "\n",
        "The second command installed and upgraded several key libraries required for our project. These include:\n",
        "\n",
        "\n",
        "*   datasets[audio] for handling and processing datasets, particularly those involving audio data.\n",
        "*   transformers for working with pre-trained models and fine-tuning them on specific tasks.\n",
        "*   accelerate to optimize the training process across multiple devices, such as GPUs.\n",
        "*   evaluate and jiwer for calculating metrics like Word Error Rate (WER) to assess the model's performance.\n",
        "*   tensorboard for tracking training progress and visualizing metrics.\n",
        "*   gradio for creating user-friendly interfaces to interact with the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcQI9vae814c"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet pip\n",
        "!pip install --upgrade --quiet datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJOCWUnXaTtL"
      },
      "source": [
        "# Using Hugging Face for Dataset, Model, and Endpoint Management\n",
        "In this step, we logged into the Hugging Face Hub to manage our datasets, models, and endpoints. We chose Hugging Face because it offers a simple and efficient interface, making it highly convenient for collaborative projects. Previously, we attempted to use Google Drive for these tasks, but it presented significant challenges. Each group member had to individually upload datasets whenever changes were made, leading to version control issues and inefficiencies. Additionally, the upload and download process was time-consuming, which slowed down our workflow.\n",
        "\n",
        "By switching to Hugging Face, we streamlined collaboration within our group. Hugging Face's centralized platform automatically synchronizes updates, ensuring that all members have access to the latest datasets and models without redundant uploads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRUQ_r2gMtuV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dve_0n8rzfn"
      },
      "source": [
        "# Data Preprocessing and Handling\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkCc5-3PZ_XG"
      },
      "source": [
        "# Loading Datasets\n",
        "To create a more expressive model, we began with the dataset provided, which was split into 90% for training and 10% for testing. This dataset was primarily focused on financial contexts, making it valuable for specialized applications but limited in its ability to generalize across diverse Twi language use cases. To address this limitation, we decided to expand the dataset by incorporating a more general Twi dataset containing 28,000 examples from the repository kojo-george/asante-twi-tts.\n",
        "\n",
        "The financial training dataset was further split into training (80%) and validation (20%) sets to evaluate model performance effectively during training. To integrate the general dataset with the financial dataset, we aligned the column names and removed any unnecessary columns to ensure consistency. This step allowed us to seamlessly combine the datasets, creating a more comprehensive and diverse dataset for training.\n",
        "\n",
        "By combining the specialized financial dataset with the larger and more general Twi dataset, we ensured that the model could better handle diverse linguistic contexts while retaining its financial specialization. This approach strikes a balance between domain-specific accuracy and general expressiveness, improving the model's overall utility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK3E9rEVLwKO"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
        "\n",
        "# Load your financial dataset\n",
        "train_dataset = load_dataset(\"Ibaahjnr/Twi_Train_Dataset\", split=\"train\")\n",
        "test_dataset = load_dataset(\"Ibaahjnr/Twi_Test_Dataset\", split=\"train\")\n",
        "\n",
        "# Split your financial training dataset into train and validation\n",
        "dataset_size = len(train_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "train_financial = train_dataset.select(range(train_size))\n",
        "val_financial = train_dataset.select(range(train_size, dataset_size))\n",
        "\n",
        "# Load the general Twi dataset\n",
        "general_dataset = load_dataset(\"kojo-george/asante-twi-tts\")\n",
        "\n",
        "# **Align Column Names Between Datasets**\n",
        "general_dataset = general_dataset.rename_column(\"text\", \"transcription\")\n",
        "general_dataset = general_dataset.remove_columns([\"file_name\"])\n",
        "\n",
        "# Check the column names of your financial dataset\n",
        "print(\"Financial Dataset Columns:\", train_financial.column_names)\n",
        "\n",
        "# Check the column names of the general dataset\n",
        "print(\"General Dataset Columns:\", general_dataset['train'].column_names)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bouDjB0Fc3Lb"
      },
      "source": [
        "# Combining and Preparing the Datasets\n",
        "In this step, we focused on preparing a unified dataset for training, validation, and testing. Since the project involves working with audio data, we first ensured consistency in the datasets by casting the audio column in all datasets to the same format using the Audio class from the datasets library. This step standardizes the audio features, making them compatible for model training and evaluation.\n",
        "\n",
        "To enhance the dataset's expressiveness, we combined the financial dataset with the general Twi dataset. The training and validation sets from the financial dataset were concatenated with the training and validation splits from the general dataset, while the testing set included samples from both datasets. The combined datasets were shuffled with a fixed seed to randomize the examples, ensuring that the model does not learn in a biased sequence. For evaluation purposes, the testing dataset was limited to 100 samples to streamline validation during development.\n",
        "\n",
        "The combined dataset, stored as a DatasetDict named common_voice, is well-structured and includes distinct splits for training, validation, and testing. This process is critical to ensure that the model is trained on a diverse and representative dataset, evaluated effectively during training, and tested on a balanced subset. By merging these datasets, we aim to build a robust model that balances domain-specific expertise with general linguistic expressiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XsIlI-Wbfg2"
      },
      "outputs": [],
      "source": [
        "from datasets import Audio\n",
        "\n",
        "# Cast 'audio' column in both datasets to ensure consistent features\n",
        "financial_train_dataset = train_financial.cast_column(\"audio\", Audio(sampling_rate=None))\n",
        "financial_test_dataset = train_financial.cast_column(\"audio\", Audio(sampling_rate=None))\n",
        "general_dataset = general_dataset.cast_column(\"audio\", Audio(sampling_rate=None))\n",
        "\n",
        "# Combine the datasets\n",
        "train_combined = concatenate_datasets([financial_train_dataset, general_dataset[\"train\"]]).shuffle(seed=42)\n",
        "val_combined = concatenate_datasets([val_financial, general_dataset[\"validation\"]]).shuffle(seed=42)\n",
        "test_combined = concatenate_datasets([financial_test_dataset, general_dataset[\"test\"]]).shuffle(seed=42)\n",
        "\n",
        "# Create the DatasetDict named 'common_voice'\n",
        "common_voice = DatasetDict({\n",
        "    \"train\": train_combined,\n",
        "    \"validation\": val_combined,\n",
        "    \"test\": test_combined.select(range(100))\n",
        "})\n",
        "\n",
        "# Print the resulting DatasetDict\n",
        "print(common_voice)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdoYfB1oa3vm"
      },
      "source": [
        "\n",
        "# Initializing the Feature Extractor\n",
        "In this step, we initialized a WhisperFeatureExtractor from the transformers library using the pre-trained openai/whisper-medium model. The feature extractor is a crucial component in processing audio data for training and inference. It transforms raw audio waveforms into a format suitable for the Whisper model, such as spectrogram representations or normalized audio features.\n",
        "\n",
        "The choice of using a pre-trained feature extractor ensures consistency with the Whisper model's architecture and pre-training configurations. This step is essential because the model expects inputs in a specific format to perform effectively. By leveraging the pre-trained WhisperFeatureExtractor, we save time and avoid potential errors from manually defining audio preprocessing steps, ensuring our data preparation aligns with the model's requirements.\n",
        "\n",
        "Using this feature extractor also helps maintain the fidelity of the audio data while enabling the model to capture both domain-specific and general acoustic patterns. This ensures that our training and evaluation processes are optimized for the Whisper model's capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmE2yRewEfRH"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvYH3DqptETB"
      },
      "source": [
        "# Initializing the Tokenizer\n",
        "In this step, we initialized the WhisperTokenizer from the transformers library using the pre-trained openai/whisper-medium model. The tokenizer plays a critical role in converting text data into numerical tokens that the model can process. While the primary goal of our model is to transcribe Twi, we faced a unique challenge: the tokenizer does not explicitly support Twi. Additionally, our dataset occasionally included English words mixed with Twi, a common linguistic phenomenon.\n",
        "\n",
        "To address this, we used the multilingual capabilities of the WhisperTokenizer while specifying language=\"English\" and task=\"transcribe\". This allowed the tokenizer to handle English words naturally, while also leveraging its multilingual support for Twi. By adopting this approach, we ensured that the model could effectively tokenize and process mixed-language transcriptions without losing important linguistic nuances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvaMSKH-s-nf"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-medium\", language=\"English\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_giRvRB0U-K"
      },
      "source": [
        "Building on the earlier steps, we initialized the WhisperProcessor from the transformers library using the pre-trained openai/whisper-medium model. The processor integrates the functionality of both the feature extractor and tokenizer, making it a unified tool for preparing audio data and handling text outputs. This ensures consistency in the preprocessing pipeline for our transcription task.\n",
        "\n",
        "As noted earlier, the primary goal of our model is to transcribe Twi, but our dataset often contains English words mixed with Twi. To address this, we specified language=\"English\" and task=\"transcribe\", leveraging the multilingual capabilities of the Whisper model. This builds on the earlier tokenizer setup, allowing the processor to handle code-switching seamlessly while preserving the integrity of both Twi and English segments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_88uy9BlTpXw"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"English\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq0C9alnbpt9"
      },
      "source": [
        "Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ7jmv36gqvu"
      },
      "outputs": [],
      "source": [
        "!pip install numba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Moa5us_bO7y_"
      },
      "outputs": [],
      "source": [
        "print(common_voice['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhtmX9fW0UZq"
      },
      "source": [
        "# Resampling Audio Data\n",
        "\n",
        "In this step, we resampled all the audio data in the common_voice dataset (train, validation, and test splits) to a standardized sampling rate of 16,000 Hz using the cast_column method from the datasets library. This step was crucial for overcoming inconsistencies in the dataset, which could arise from variations in the sampling rates of audio files collected from different sources.\n",
        "\n",
        "One of the major challenges we faced earlier was dealing with datasets that had varying audio characteristics, including differing sampling rates. This inconsistency led to issues during preprocessing, as the Whisper model's feature extractor expects audio input with a fixed sampling rate. Without standardization, the model's performance could degrade due to mismatched input features or processing errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgqfwraBbrh1"
      },
      "outputs": [],
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice['train'] = common_voice['train'].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "common_voice['validation'] = common_voice['validation'].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "common_voice['test'] = common_voice['test'].cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFqg34arcEKm"
      },
      "outputs": [],
      "source": [
        "print(common_voice[\"train\"][0])\n",
        "print(common_voice[\"validation\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiPsmKbgslnx"
      },
      "source": [
        "The prepare_dataset function processes a batch of data to prepare it for training with the Whisper model. It takes a single batch of data as input and performs two key operations.\n",
        "\n",
        "First, it computes log-Mel spectrogram input features from the audio data. The function extracts the audio array and its sampling rate from the audio field of the batch and passes them to the feature_extractor. This extracts the spectrogram features, which are stored in the batch under the key input_features. These features represent the audio in a form that the model can process effectively.\n",
        "\n",
        "Second, the function tokenizes the transcription text. It uses the tokenizer to encode the transcription field into a sequence of label IDs, which are stored in the batch under the key labels. These labels represent the text in a numerical format compatible with the model's output during training.\n",
        "\n",
        "By performing these operations, the function ensures that each batch contains both the input features and corresponding labels needed for training the model. It returns the processed batch for further use in the training pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1oeAGqccE1Y"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IFQfhgxtVad"
      },
      "source": [
        "# Mapping the prepare_dataset Function to the Dataset\n",
        "The line of code applies the prepare_dataset function to all splits (train, validation, test) of the common_voice dataset using the map method. This step transforms the dataset into a format that is ready for training the Whisper model.\n",
        "\n",
        "Hereâ€™s a breakdown of what this does:\n",
        "\n",
        "Applying the Transformation:\n",
        "\n",
        "The prepare_dataset function is applied to each batch of the dataset. It preprocesses the audio data into log-Mel spectrogram features and tokenizes the transcriptions into numerical label IDs, which the model requires for training.\n",
        "Removing Unnecessary Columns:\n",
        "\n",
        "The remove_columns argument removes all existing columns in the dataset that are no longer needed after processing. It uses the column names from the train split (common_voice.column_names[\"train\"]). This ensures that the dataset only contains the processed input_features and labels columns, reducing redundancy and simplifying the data structure.\n",
        "Parallel Processing:\n",
        "\n",
        "The num_proc=4 argument enables multiprocessing, allowing the dataset to be processed in parallel using 4 processes. This significantly speeds up the preprocessing step, especially for large datasets.\n",
        "By applying this transformation, the common_voice dataset is fully prepared for training. Each example in the dataset is now in the format expected by the Whisper model, containing the extracted audio features (input_features) and the corresponding tokenized transcriptions (labels). This step is crucial for ensuring the data pipeline runs efficiently and effectively during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpYjcvtOcUzJ"
      },
      "outputs": [],
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jrmAFILcp4Z"
      },
      "source": [
        "# Model\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrGEYu9vcpDi"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BrtK_X7cvJu"
      },
      "outputs": [],
      "source": [
        "model.generation_config.language = \"English\"\n",
        "model.generation_config.task = \"transcribe\"\n",
        "\n",
        "model.generation_config.forced_decoder_ids = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XADxOmSEc0U9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mrLeKetc8EL"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N6X-EgEc9U3"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")\n",
        "metric2 = evaluate.load(\"cer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pJl67j0c_Bf"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "    cer = 100 * metric2.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer, \"cer\": cer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyqnea_6dBsf"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./Asanti_Twi_Model_V2.1\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=3000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB5n-4ZSdWP3"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJcCTK_2dYKj"
      },
      "outputs": [],
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChYZ23A6dZ22"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkKYN1xddd0v"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c704f91e-241b-48c9-b8e0-f0da396a9663"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"Isbaahjnr/Twi_Train_Dataset\",  # Ensure this dataset exists on Hugging Face Hub\n",
        "    \"dataset\": \"Twi_Train_Dataset\",  # Human-readable name for the dataset\n",
        "    \"dataset_args\": '{\"config\": \"audio translation\", \"split\": \"train\"}',  # Valid JSON-like string for dataset arguments\n",
        "    \"language\": [\"twi\"],  # Language in lowercase\n",
        "    \"model_name\": \"Twi_Whisper\",  # Human-readable name for the model\n",
        "    \"finetuned_from\": \"openai/whisper-medium\",  # Ensure this is correct\n",
        "    \"tasks\": [\"automatic-speech-recognition\"],  # Valid task name\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKBHl4wEjkY1"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EIiUukHHIfw"
      },
      "source": [
        "Testing the model on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3ZjAujkSSiT"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngZasVS3mLeG"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCBrjdGMHMWC"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import torch\n",
        "import torchaudio\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "# Load the Whisper model and processor\n",
        "model_name = \"Ibaahjnr/Asanti_Twi_Model_V2.1\"  # Replace with your Hugging Face model path\n",
        "processor = WhisperProcessor.from_pretrained(model_name)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Load the dataset\n",
        "dataset_name = \"Ibaahjnr/Twi_Test_Dataset\"  # Replace with your dataset path\n",
        "test_dataset = load_dataset(dataset_name, split=\"train\")  # Assuming your test split is named \"test\"\n",
        "\n",
        "# Randomly select 10 samples from the test dataset\n",
        "test_samples = random.sample(list(test_dataset), 10)\n",
        "\n",
        "# Load WER and CER metrics from the evaluate library\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "cer_metric = evaluate.load(\"cer\")\n",
        "\n",
        "# Prepare evaluation\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "# Resample the audio to 16000 Hz\n",
        "def resample_audio(audio, target_sampling_rate=16000):\n",
        "    waveform = torch.tensor(audio[\"array\"], dtype=torch.float32)  # Ensure the tensor is float32\n",
        "    resampled_waveform = torchaudio.transforms.Resample(\n",
        "        orig_freq=audio[\"sampling_rate\"], new_freq=target_sampling_rate\n",
        "    )(waveform)\n",
        "    return resampled_waveform.numpy()\n",
        "\n",
        "# Iterate through the randomly selected test samples\n",
        "for sample in tqdm(test_samples, desc=\"Processing Audio Samples\", unit=\"sample\"):\n",
        "    # Resample audio and load target transcription\n",
        "    audio = sample['audio']\n",
        "    resampled_audio = resample_audio(audio)\n",
        "    reference_text = sample['transcription']  # Replace 'transcription' with your dataset's transcription column name\n",
        "\n",
        "    # Process the resampled audio\n",
        "    inputs = processor(resampled_audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        predicted_ids = model.generate(inputs[\"input_features\"])\n",
        "\n",
        "    # Decode the prediction\n",
        "    predicted_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Append for metric computation\n",
        "    predictions.append(predicted_text)\n",
        "    references.append(reference_text)\n",
        "\n",
        "# Print predictions and references to verify\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"References:\", references)\n",
        "\n",
        "# Compute WER and CER\n",
        "wer_score = wer_metric.compute(predictions=predictions, references=references)\n",
        "cer_score = cer_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(f\"Word Error Rate (WER): {wer_score:.2f}\")\n",
        "print(f\"Character Error Rate (CER): {cer_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIM3Eb3ReFI0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# displaying the 10 sentences for our held out test data\n",
        "heldout = pd.DataFrame({\"predictions\":predictions,\"references\":references})\n",
        "heldout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9LDm7KKcaMh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset\n",
        "dataset_name = \"Ibaahjnr/Asante_Twi_Collected_Test\"  # Replace with your dataset path\n",
        "test_dataset = load_dataset(dataset_name, split=\"train\")  # Assuming your test split is named \"test\"\n",
        "\n",
        "# Randomly select 10 samples from the test dataset\n",
        "test_samples = random.sample(list(test_dataset), 10)\n",
        "\n",
        "# Load WER and CER metrics from the evaluate library\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "cer_metric = evaluate.load(\"cer\")\n",
        "\n",
        "# Prepare evaluation\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "# Resample the audio to 16000 Hz\n",
        "def resample_audio(audio, target_sampling_rate=16000):\n",
        "    waveform = torch.tensor(audio[\"array\"], dtype=torch.float32)  # Ensure the tensor is float32\n",
        "    resampled_waveform = torchaudio.transforms.Resample(\n",
        "        orig_freq=audio[\"sampling_rate\"], new_freq=target_sampling_rate\n",
        "    )(waveform)\n",
        "    return resampled_waveform.numpy()\n",
        "\n",
        "# Iterate through the randomly selected test samples\n",
        "for sample in tqdm(test_samples, desc=\"Processing Audio Samples\", unit=\"sample\"):\n",
        "    # Resample audio and load target transcription\n",
        "    audio = sample['audio']\n",
        "    resampled_audio = resample_audio(audio)\n",
        "    reference_text = sample['transcription']  # Replace 'transcription' with your dataset's transcription column name\n",
        "\n",
        "    # Process the resampled audio\n",
        "    inputs = processor(resampled_audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        predicted_ids = model.generate(inputs[\"input_features\"])\n",
        "\n",
        "    # Decode the prediction\n",
        "    predicted_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Append for metric computation\n",
        "    predictions.append(predicted_text)\n",
        "    references.append(reference_text)\n",
        "\n",
        "# Print predictions and references to verify\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"References:\", references)\n",
        "\n",
        "# Compute WER and CER\n",
        "wer_score = wer_metric.compute(predictions=predictions, references=references)\n",
        "cer_score = cer_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(f\"Word Error Rate (WER): {wer_score:.2f}\")\n",
        "print(f\"Character Error Rate (CER): {cer_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXy4l8cGWFL4"
      },
      "outputs": [],
      "source": [
        "# displaying the 10 sentences for our compiled data\n",
        "our_test = pd.DataFrame({\"predictions\":predictions,\"references\":references})\n",
        "our_test"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
